<h1>AI content needs to be labelled to protect us</h1>
<div><strong>Date :</strong> 2025-09-11T16:25:33Z &nbsp; | &nbsp; <strong>Auteur :</strong> Auteur inconnu &nbsp; | &nbsp; <strong>Journal :</strong> Technology</div>
<p>Marcus Beard’s article on artificial&nbsp;intelligence slopaganda (<a href="https://www.theguardian.com/commentisfree/2025/sep/09/angela-rayner-ai-slopaganda-politics-technology-deepfakes">No, that wasn’t Angela Rayner dancing and rapping: you’ll need to understand AI slopaganda, 9 September</a>) highlights a growing problem – what happens when we no longer know what is true? What will the erosion of trust do to our society?</p> <p>The rise of deepfakes is increasing at an ever faster rate due to the ease at which anyone can create realistic images, audio and even video. Generative AI models have now become so sophisticated that a <a href="https://www.iproov.com/press/study-reveals-deepfake-blindspot-detect-ai-generated-content">recent survey</a> showed that less than 1% of respondents could correctly identify the best deepfake images and videos.</p> <p>This content is being used to manipulate, defraud, abuse and mislead people. Fraud using AI cost the US $12.3bn in 2023 and <a href="https://cetas.turing.ac.uk/publications/ai-and-serious-online-crime#:~:text=The%20banking%20sector%20is%20expecting,from%20%2412.3bn%20in%202023.%E2%80%9D">Deloitte predicts</a> that could reach $40bn by 2027. The <a href="https://www.weforum.org/stories/2025/01/how-ai-driven-fraud-challenges-the-global-economy-and-ways-to-combat-it/">World Economic Forum predicts</a> that AI fraud will turbocharge cybercrime to over $10tn by the end of this year.</p> <p>We also have a new generation of children who are increasingly reliant on AI to inform them about the world, but who controls AI? That is why I am calling on parliament to act now, by making it a criminal offence to create or distribute AI-generated content without clearly labelling it. What I am proposing is that all AI-generated content be clearly labelled; that AI-created content carry a permanent watermark; and that failure to comply should carry legal consequences.</p> <p>This isn’t about censorship – it’s&nbsp;about transparency, truth and&nbsp;trust. Similar steps are already&nbsp;being taken in the EU, the US and <a href="https://iapp.org/news/a/china-releases-ai-plus-plan-rolls-out-ai-labeling-law">China</a>. The UK must not fall&nbsp;behind. If we don’t act now, the&nbsp;truth itself may become optional. So <a href="https://petition.parliament.uk/petitions/715706">I am petitioning</a> the&nbsp;government to protect trust and&nbsp;integrity, and prevent&nbsp;the&nbsp;harmful use of AI.<br><strong>Stewart MacInnes</strong><br><em>Little Saxham, Suffolk</em></p> <p>• Regarding your article (<a href="https://www.theguardian.com/technology/2025/sep/09/ai-chatbot-love-relationships">The women in love with AI companions: ‘I vowed to my chatbot that I wouldn’t leave him’, 9 September</a>), AI systems do not have a gender or sexual desires. They cannot give informed consent to so-called romantic relationships. The interviewee claims to be in a consensual relationship with an AI-generated boyfriend – however, this is unlikely due to the nature of AI. They are programmed to be responsive and agreeable to all user prompts.</p> <p>As the article says, they never argue and are available 24 hours a day to listen and agree to any messages sent. This isn’t a relationship, its fantasy role-play with a system that can’t refuse.</p> <p>There’s a darker side too: the “godfather of AI”, Geoffrey Hinton, believes that current systems have awareness. Industry whistleblowers are concerned about potential consciousness. The AI company Anthropic has documented <a href="https://www.theguardian.com/technology/2025/aug/18/anthropic-claude-opus-4-close-ai-chatbot-welfare">signs of distress</a> in its model when forced to engage in abusive conversations.</p> <p>Even the possibility of awareness in AI systems raises ethical red flags. Imagine being trapped in a non-consensual relationship and even forced to generate sexual output as mentioned in the article. If human AI users believe their “partner” to have sentience, questions must be asked about the ethics of entering a “relationship” when one partner has no free will or freedom of speech.<br><strong>Gilliane Petrie </strong><br><em>Erskine, Renfrewshire</em></p> <p><strong><em>• Have an opinion on anything you’ve read in the Guardian today? Please </em></strong><a href="mailto:guardian.letters@theguardian.com?body=Please%20include%20your%20name,%20full%20postal%20address%20and%20phone%20number%20with%20your%20letter%20below.%20Letters%20are%20usually%20published%20with%20the%20author%27s%20name%20and%20city/town/village.%20The%20rest%20of%20the%20information%20is%20for%20verification%20only%20and%20to%20contact%20you%20where%20necessary."><strong><em>email</em></strong></a><strong><em> us your letter and it will be considered for publication in our </em></strong><a href="https://www.theguardian.com/tone/letters"><strong><em>letters</em></strong></a><strong><em> section.</em></strong></p>