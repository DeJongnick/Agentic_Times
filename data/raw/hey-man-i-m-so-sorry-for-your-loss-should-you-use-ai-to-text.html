<h1>‘Hey man, I’m so sorry for your loss’: should you use AI to text?</h1>
<div><strong>Date :</strong> 2025-06-30T16:00:40Z &nbsp; | &nbsp; <strong>Auteur :</strong> Adrienne Matei &nbsp; | &nbsp; <strong>Journal :</strong> Wellness</div>
<figure class="element element-atom"> <gu-atom data-atom-id="interactives/2023/01/interactive-article-structure/portrait-image-mainmedia-feature" data-atom-type="interactive"> 
  <div>
   default
  </div>
 </gu-atom> 
</figure> 
<p>Earlier this spring, Nik Vassev heard a high school friend’s mother had died. Vassev, a 32-year-old tech entrepreneur in Vancouver, Canada, opened up Claude AI, Anthropic’s <a href="https://www.theguardian.com/technology/artificialintelligenceai">artificial intelligence</a> chatbot.</p> 
<p>“My friend’s mom passed away and I’m trying to find the right way to be there for him and send him a message of support like a good friend,” he typed.</p> 
<p>Vassev mostly uses AI to answer work emails, but also for personal communications. “I just wanted to just get a second opinion about how to approach that situation,” he says. “As guys, sometimes we have trouble expressing our emotions.”</p> 
<p>Claude helped Vassev craft a note: “Hey man, I’m so sorry for your loss. Sending you and your family lots of love and support during this difficult time. I’m here for you if you need anything …” it read.</p> 
<p>Thanks to the message, Vassev’s friend opened up about their grief. But Vassev never revealed that AI was involved. People “devalue” writing that is AI-assisted, he acknowledges. “It can rub people the wrong way.”</p> 
<aside class="element element-rich-link element--thumbnail"> 
 <p> <span>Related:</span><a href="https://www.theguardian.com/wellness/2025/jun/18/suppressing-anger-unhealthy">Is it true that I ‘don’t get angry’? Or am I actually dangerously suppressing it?</a> </p> 
</aside> 
<p>Vassev learned this lesson because a friend once called him out for relying heavily on AI during an argument: “Nik, I want to hear your voice, not what ChatGPT has to say.” That experience left Vassev chastened. Since then, he’s been trying to be more sparing and subtle, “thinking for myself and having AI assist”, he says.</p> 
<p>Since late 2022, AI adoption has exploded in professional contexts, where it’s used as a <a href="https://www.theguardian.com/technology/2024/nov/17/have-your-bot-speak-to-my-bot-can-ai-productivity-apps-turbocharge-my-life">productivity-boosting</a> tool, and among <a href="https://www.theguardian.com/technology/2025/jun/17/universities-face-a-reckoning-on-chatgpt-cheats">students,</a> <a href="https://www.theguardian.com/education/2025/jun/15/thousands-of-uk-university-students-caught-cheating-using-ai-artificial-intelligence-survey">who increasingly</a> <a href="https://www.theguardian.com/technology/2024/dec/15/i-received-a-first-but-it-felt-tainted-and-undeserved-inside-the-university-ai-cheating-crisis">use chatbots to cheat</a>.</p> 
<p>Yet AI is becoming the invisible infrastructure of personal communications, too – punching up <a href="https://www.theguardian.com/technology/2024/dec/03/the-chatgpt-secret-is-that-text-message-from-your-friend-your-lover-or-a-robot">text messages</a>, birthday cards and <a href="https://www.nytimes.com/2025/06/13/technology/chatgpt-ai-chatbots-conspiracies.html">obituaries</a>, even though we associate such compositions with “from the heart” authenticity.</p> 
<p>Disclosing the role of AI could defeat the purpose of these writings, which is to build trust and express care. Nonetheless, one person anonymously told me that he used ChatGPT while writing his father of the bride speech; another wished OpenAI had been around when he had written his vows because it would have “saved [him] a lot of time”. Online, a Redditor <a href="https://www.reddit.com/r/TrueOffMyChest/comments/121w8pc/i_used_chatgbt_to_write_my_moms_birthday_card/">shared that they used</a> ChatGPT to write their mom’s birthday card: “She not only cried, she keeps it on her side table and reads [it] over and over, every day since I gave it to her,” they wrote. “I can never tell her.”</p> 
<p>Research about transparency and AI use mostly focuses on <a href="https://www.theguardian.com/money/2025/apr/01/how-to-use-ai-job-interview-salary-research-employer">professional</a> <a href="https://www.theguardian.com/technology/2025/mar/15/its-happening-fast-creative-workers-and-professionals-share-their-fears-and-hopes-about-the-rise-of-ai">settings</a>, where <a href="https://www.gallup.com/workplace/691643/work-nearly-doubled-two-years.aspx">40%</a> of US workers use the tools. However, a recent <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-105003205020&amp;origin=inward&amp;txGid=118a58b75cb0f4b812cff95db3a23164">study</a> from the University of Arizona concluded that “AI disclosure can harm social perceptions” of the disclosers at work, and similar findings apply to personal relationships.</p> 
<p>In one <a href="https://www.sciencedaily.com/releases/2023/09/230911141009.htm">2023 study</a>, 208 adults received a “thoughtful” note from a friend; those who were told the note was written with AI felt less satisfied and “more uncertain about where they stand” with the friend, <a href="https://www.sciencedaily.com/releases/2023/09/230911141009.htm">according to</a> Bingjie Liu, the lead author of the study and an assistant professor of communication at Ohio State University.</p> 
<p>On subreddits such as <a href="https://www.reddit.com/r/AmIOverreacting/comments/1iwu3cn/aio_did_my_boyfriend_just_use_ai_to_text_me/">r/AmIOverreacting</a> or r/Relationship_advice, it’s easy to find users expressing distress upon discovering, say, that their husband used ChatGPT to <a href="https://www.reddit.com/r/relationship_advice/comments/1d9tw9j/my_30f_husband_36m_used_ai_to_write_his_vows_how/">write their wedding vows</a>. (“To me, these words are some of the most important that we will ever say to each other. I feel so sad knowing that they weren’t even his own.”)</p> 
<aside class="element element-pullquote element--supporting"> 
 <blockquote> 
  <p>If I heard that you were sending me an email and making it sound more empathetic than you really were, I wouldn’t let it go</p> 
  <footer> <cite>Dr Vanessa Urch Druskat</cite> 
  </footer> 
 </blockquote> 
</aside> 
<p>AI-assisted personal messages can convey that the sender didn’t want to bother with sincerity, says <a href="https://www.vanessadruskat.com/">Dr Vanessa Urch Druskat</a>, a social and organizational psychologist and professor specializing in emotional intelligence. “If I heard that you were sending me an email and making it sound more empathetic than you really were, I wouldn’t let it go,” she says.</p> 
<p>“There’s a baseline expectation that our personal communications are authentic,” says Druskat. “We’re wired to pick up on inauthenticity, disrespect – it feels terrible,” she says.</p> 
<p>But not everyone draws the same line when it comes to how much AI involvement is tolerable or what constitutes deceit by omission. Curious, I conducted an informal social media poll among my friends: if I used AI to write their whole birthday card, how would they feel? About two-thirds said they would be “upset”; the rest said it would be fine. But if I had used AI only in a supplementary role – say, some editing to hit the right tone – the results were closer to 50-50.</p> 
<p>Using AI in personal messages is a double gamble: first, that the recipient won’t notice, and second, that they won’t mind. Still, there are arguments for why taking the risk is worthwhile, and why a <a href="https://www.theguardian.com/technology/2024/dec/30/dating-apps-prepare-to-launch-ai-features-to-help-users-find-love">hint of AI in a</a> <a href="https://www.theguardian.com/lifeandstyle/2025/mar/08/ai-wingmen-bots-to-write-profiles-and-flirt-on-dating-apps">Hinge message</a> might not be so bad. For instance, AI can be helpful for bridging <a href="https://www.sciencedirect.com/science/article/pii/S2666920X24000316">communication gaps</a> rooted in cultural, linguistic or other forms of diversity.</p> 
<p>Plus, personal messages have never been totally spontaneous and original. People routinely seek advice from friends, therapists or <a href="https://www.reddit.com/r/namenerds/comments/1j6vsxg/wife_wants_to_name_our_twins_romeo_and_juliet/#:~:text=You%20could%20even%20do%20both,to%20be%20made%20jokes%20of.">strangers</a> about disagreements, delicate conversations or important notes. Greeting cards have long come with pre-written sentiments (although Mother’s Day founder Anna Jarvis once <a href="https://www.britannica.com/biography/Anna-Jarvis">scolded</a> that printed cards were “lazy”).</p> 
<p>Sara Jane Ho, an etiquette expert, says she has used ChatGPT “in situations where I’ve been like: ‘Change this copy to make it more heartfelt.’ And it’s great copy.”</p> 
<p>Ho argues that using ChatGPT to craft a personal message actually shows “a level of consideration”.</p> 
<figure class="element element-interactive interactive" data-interactive="https://interactive.guim.co.uk/embed/iframe-wrapper/0.1/boot.js" data-canonical-url="https://interactive.guim.co.uk/uploader/embed/2023/10/archive-zip/giv-13425WMrLo2pc9VIk/" data-alt="Graphic with three lines of text that say, in bold, ‘Well Actually’, then ‘Read more on living a good life in a complex world,’ then a pinkish-lavender pill-shaped button with white letters that say ‘More from this section’"> <a href="https://interactive.guim.co.uk/uploader/embed/2023/10/archive-zip/giv-13425WMrLo2pc9VIk/">Interactive</a> 
</figure> 
<p>Expressing sensitivity helps build relationships, and it makes sense that people who struggle with words would appreciate assistance. Calculators are standard digital tools; why not chatbots? “I always say that the spirit of etiquette is about putting others at ease,” she says. “If the end result is something that is nice for the other person and that shows respect or consideration or care, then they don’t need to see how the sausage is made.”</p> 
<p>I asked Ho what she would say to a person upset by an AI-assisted note. “I’d ask them: ‘Why are you so easily offended?’” Ho says.</p> 
<p>Plus, she says using AI is convenient and fast. “Why would you make yourself walk someplace if you have a car?” she asks.</p> 
<p>***</p> 
<p>Increasingly, people are drifting through digitized lives that reject “the very notion that engagement should require effort”, at perceiving less value in character building and experiences like “working hard” and “learning well”, author and educator Kyla Scanlon <a href="https://kyla.substack.com/p/the-most-valuable-commodity-in-the?_bhlid=8d11a3b3dc69ff5f2f6afb45c7aae9347a1ba2d7">argued in an essay</a> last month. This bias toward effortlessness characterizes the emotional work of relationships as burdensome, even though it helps create intimacy.</p> 
<p>“People have sort of conditioned themselves to want a completely seamless and frictionless experience in their everyday lives 100% of the time,” says Josh Lora, a writer and sociologist who has written <a href="https://tellthebeees.substack.com/p/deus-ex-machina">about AI and loneliness</a>. “There are people who Uber everywhere, who Seamless everything, who Amazon everything, and render their lives completely smooth.”</p> 
<p>Amid this <a href="https://www.theguardian.com/books/2024/nov/04/the-big-idea-is-convenience-making-our-lives-more-difficult">convenience-maxxing</a>, AI figures as an efficient way out of relational labor, or small mistakes, tensions and inadequacies in communication, says Lora.</p> 
<aside class="element element-rich-link element--thumbnail"> 
 <p> <span>Related:</span><a href="https://www.theguardian.com/wellness/article/2024/aug/02/ai-friend-wearable-companion-loneliness">Can an AI friend make you less lonely?</a> </p> 
</aside> 
<p>We use language to be understood or co-create a sense of self. “So much of our experience as people is rendered in the struggle to make meaning, to self actualize, to explain yourself to another person,” Lora says.</p> 
<p>But when we outsource that labor to a chatbot, we lose out on developing self-expression, nuanced social skills, and emotional intelligence. We also lose out on the feelings of interpersonal gratitude that arise from taking the time to write kindly to our loved ones, as one <a href="https://link.springer.com/article/10.1007/s42761-022-00160-3">2023 study</a> from the University of California, Riverside, found.</p> 
<p>Many people already approach life as a series of objectives: get good grades, get a job, earn money, get married. In that mindset, a relationship can feel like something to manage effectively rather than a space of mutual recognition. What happens if it <a href="https://www.theguardian.com/wellness/2024/feb/21/when-end-friendships-toxic-self-care">stops feeling worth the effort</a>?</p> 
<p>Summer (who requested a pseudonym for privacy), a 30-year-old university tutor, said she became best friends with Natasha (also a pseudonym) while pursuing their respective doctoral degrees. They lived four hours apart, and much of their relationship unfolded in long text message exchanges, debating ideas or analyzing people they knew.</p> 
<aside class="element element-pullquote element--supporting"> 
 <blockquote> 
  <p>If the end result is something that is nice for the other person ... then they don’t need to see how the sausage is made</p> 
  <footer> <cite>Sara Jane Ho, etiquette expert</cite> 
  </footer> 
 </blockquote> 
</aside> 
<p>About a year ago, Natasha began to use ChatGPT to help with work tasks. Summer said she quickly seemed deeply enamoured with AI’s speed and fluency. (Researchers have warned the technology can be <a href="https://www.technologyreview.com/2024/08/05/1095600/we-need-to-prepare-for-addictive-intelligence/">addictive</a>, to the detriment of human social engagement.) Soon, subtle tone and content changes led Summer to suspect Natasha was using AI in their personal messages. (Natasha did not respond to a request for comment.)</p> 
<p>After six years of lively intellectual curiosity, their communication dwindled. Occasionally, Natasha asked Summer for her opinion on something, then disappeared for days. Summer felt like <em>she</em> was the third party to a deep conversation happening between her best friend and a machine. “I’d engage with her as a friend, a whole human being, and she’d engage with me as an obstacle to this meaning-making machine of hers,” Summer tells me.</p> 
<p>Summer finally called Natasha to discuss how AI use was affecting their friendship. She felt Natasha was exchanging the messy imperfections of rambling debate for an emotionally bankrupt facsimile of ultra-efficient communication. Natasha didn’t deny using chatbots, and “seemed to always have a reason” for continuing despite Summer’s moral and intellectual qualms.</p> 
<p>Summer “felt betrayed” that a close friend had used AI as “an auxiliary” to talk to her. “She couldn’t find the inherent meaning in us having an exchange as people,” she says. To her, adding AI into relationships “presupposes inadequacy” in them, and offers a sterile alternative: always saying the right thing, back and forth, frictionless forever.</p> 
<p>The two women are no longer friends.</p> 
<p>“What you’re giving away when you engage in too much convenience is your humanity, and it’s creepy to me,” Summer says.</p> 
<p>***</p> 
<p>Dr Mathieu Corteel is a philosopher and author of a book grappling with the implications of AI (only available <a href="https://www.leslibraires.ca/en/books/ni-dieu-ni-ia-mathieu-corteel-9782348084614.html?srsltid=AfmBOoo3QKVFzKeANMzPh_kcFhbXL6je3Q-zdLVMJo0STZYqw0yAMGti">in French</a>) as a game we have all entered without “knowing the rules”.</p> 
<p>Corteel is not anti-AI, but believes that overreliance on it alienates us from our own judgment, and by extension, humanity – “which is why I consider it as one of the most important philosophical problems we are facing right now”, he says.</p> 
<p>If a couple, for example, expressed love through AI-generated poems, they would be skipping crucial steps of meaning-making to create “a combination of symbols” absent of meaning, he says. You can interpret meaning retrospectively, reading intent into an AI’s output, “but that’s just an effect”, he says.</p> 
<aside class="element element-rich-link element--thumbnail"> 
 <p> <span>Related:</span><a href="https://www.theguardian.com/wellness/2025/jun/17/what-is-metabolic-syndrome">What is metabolic syndrome – and do we really need to worry about it?</a> </p> 
</aside> 
<p>“AI is unable to give meaning to something because it’s outside of the semantics produced by human beings, by human culture, by human interrelation, the social world,” says Corteel.</p> 
<p>If AI can churn out convincingly heartfelt words, perhaps even our most intimate expressions have always been less special than we had hoped. Or, as the tech theorist Bogna Konior <a href="https://www.sum.si/journal-articles/angelsexual-chatbot-celibacy-and-other-erotic-suspensions">recently wrote</a>: “What chatbots ultimately teach us is that language ain’t all that.”</p> 
<p>Corteel agrees that language is inherently flawed; we can never fully express our feelings, only try. But that gap between feeling and expression is where love and meaning live. The very act of striving to shrink that distance helps define those thoughts and feelings. AI, by contrast, offers a slick way to bypass that effort. Without the time it takes to reflect on our relationships, the struggle to find words, the practice of communicating, what are we exchanging?</p> 
<p>“We want to finish quickly with everything,” says Corteel. “We want to just write a prompt and have it done. And there’s something that we are losing – it’s the process. And in the process, there’s many important aspects. It is the co-construction of ourselves with our activities,” he says. “We are forgetting the importance of the exercise.”</p>