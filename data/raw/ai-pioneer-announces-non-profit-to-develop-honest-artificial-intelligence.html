<h1>AI pioneer announces non-profit to develop ‘honest’ artificial intelligence</h1>
<div><strong>Date :</strong> 2025-06-03T04:00:22Z &nbsp; | &nbsp; <strong>Auteur :</strong> Dan Milmo Global technology editor &nbsp; | &nbsp; <strong>Journal :</strong> Technology</div>
<p>An <a href="https://www.theguardian.com/technology/2025/jan/29/deepseek-artificial-intelligence-ai-safety-risk-yoshua-bengio">artificial intelligence pioneer</a> has launched a non-profit dedicated to developing an “honest” AI that will spot rogue systems attempting to deceive humans.</p> <p>Yoshua Bengio, a renowned computer scientist described as one of the “godfathers” of AI, will be president of LawZero, an organisation committed to the safe design of the cutting-edge technology that has <a href="https://www.theguardian.com/technology/article/2024/jul/26/why-zuckerbergs-multi-billion-dollar-gamble-doesnt-just-matter-to-meta">sparked a $1tn (£740bn) arms race</a>.</p> <p>Starting with funding of approximately $30m and more than a dozen researchers, Bengio is developing a system called Scientist AI that will act as a guardrail against AI agents – <a href="https://www.theguardian.com/technology/2024/oct/21/microsoft-launches-ai-employees-that-can-perform-some-business-tasks">which carry out tasks without human intervention</a> – showing deceptive or self-preserving behaviour, such as trying to avoid being turned off.</p> <p>Describing the current suite of AI agents as “actors” seeking to imitate humans and please users, he said the Scientist AI system would be more like a “psychologist” that can understand and predict bad behaviour.</p> <aside class="element element-rich-link element--thumbnail"> <p> <span>Related: </span><a href="https://www.theguardian.com/technology/2025/may/31/the-workers-who-lost-their-jobs-to-ai-chatgpt">‘One day I overheard my boss saying: just put it in ChatGPT’: the workers who lost their jobs to AI</a> </p> </aside>  <p>“We want to build AIs that will be honest and not deceptive,” Bengio said.</p> <p>He added: “It is theoretically possible to imagine machines that have no self, no goal for themselves, that are just pure knowledge machines – like a scientist who knows a lot of stuff.”</p> <p>However, unlike current generative AI tools, Bengio’s system will not give definitive answers and will instead give probabilities for whether an answer is correct.</p> <p>“It has a sense of humility that it isn’t sure about the answer,” he said.</p> <p>Deployed alongside an AI agent, Bengio’s model would flag potentially harmful behaviour by an autonomous system – having gauged the probability of its actions causing harm.</p> <p>Scientist AI will “predict the probability that an agent’s actions will lead to harm” and, if that probability is above a certain threshold, that agent’s proposed action will then be blocked.</p> <p>LawZero’s initial backers include AI safety body the Future of Life Institute, Jaan Tallinn, a founding engineer of Skype, and Schmidt Sciences, a research body founded by former Google chief executive Eric Schmidt.</p> <p>Bengio said the first step for LawZero would be demonstrating that the methodology behind the concept works – and then persuading companies or governments to support larger, more powerful versions. Open-source AI models, which are freely available to deploy and adapt, would be the starting point for training LawZero’s systems, Bengio added.</p> <p>“The point is to demonstrate the methodology so that then we can convince either donors or governments or AI labs to put the resources that are needed to train this at the same scale as the current frontier AIs. It is really important that the guardrail AI be at least as smart as the AI agent that it is trying to monitor and control,” he said.</p> <p>Bengio, a professor at the University of Montreal, earned the “godfather” moniker after sharing the 2018 Turing award – seen as the equivalent of a Nobel prize for computing – with Geoffrey Hinton, himself a subsequent Nobel winner, and Yann LeCun, the chief AI scientist at Mark Zuckerberg’s Meta.</p> <p>A leading voice on AI safety, he chaired the recent <a href="https://www.theguardian.com/technology/2025/jan/29/what-international-ai-safety-report-says-jobs-climate-cyberwar-deepfakes-extinction">International AI Safety report</a>, which warned that autonomous agents could cause “severe” disruption if they become “capable of completing longer sequences of tasks without human supervision”.</p> <p>Bengio said he was concerned by Anthropic’s recent admission that its latest system could <a href="https://www-cdn.anthropic.com/4263b940cabb546aa0e3283f35b686f4f3b2ff47.pdf">attempt to blackmail engineers attempting to shut it down</a>. He also pointed to research showing that AI models <a href="https://arxiv.org/abs/2412.04984">are capable of hiding their true capabilities and objectives</a>. These examples showed the world is heading towards “more and more dangerous territory” with AIs that are able to reason better, said Bengio.</p>