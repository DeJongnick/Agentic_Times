<h1>Impact of chatbots on mental health is warning over future of AI, expert says</h1>
<div><strong>Date :</strong> 2025-09-08T05:00:42Z &nbsp; | &nbsp; <strong>Auteur :</strong> Dan Milmo Global technology editor &nbsp; | &nbsp; <strong>Journal :</strong> Technology</div>
<p>The unforeseen impact of chatbots on mental health should be viewed as a warning over the existential threat posed by <a href="https://www.theguardian.com/technology/2025/may/10/ai-firms-urged-to-calculate-existential-threat-amid-fears-it-could-escape-human-control">super-intelligent artificial intelligence systems</a>, according to a prominent voice in AI safety.</p> <p>Nate Soares, a co-author of a new book on highly advanced AI titled If Anyone Builds It, Everyone Dies, said the example of Adam Raine, a US teenager who killed himself after months of conversations with the ChatGPT chatbot, underlined fundamental problems with controlling the technology.</p> <p>“These AIs, when they’re engaging with teenagers in this way that drives them to suicide – that is not a behaviour the creators wanted. That is not a behaviour the creators intended,” he said.</p> <p>He added: “Adam Raine’s case illustrates the seed of a problem that would grow catastrophic if these AIs grow smarter.”</p>    <p>Soares, a former Google and Microsoft engineer who is now president of the US-based Machine Intelligence Research Institute, warned that humanity would be wiped out if it created artificial super-intelligence (ASI), a theoretical state where an AI system is superior to humans at all intellectual tasks. Soares and his co-author, Eliezer Yudkowsky, are among the AI experts warning that such systems would not act in humanity’s interests.</p> <p>“The issue here is that AI companies try to make their AIs drive towards helpfulness and not causing harm,” said Soares. “They actually get AIs that are driven towards some stranger thing. And that should be seen as a warning about future super-intelligences that will do things nobody asked for and nobody meant.”</p> <p>In one scenario portrayed in Soares and Yudkowsky’s book, which will be published this month, an AI system called Sable spreads across the internet, manipulates humans, develops synthetic viruses and eventually becomes super-intelligent – and kills humanity as a side-effect while repurposing the planet to meet its aims.</p> <p>Some experts play down the potential threat of AI to humanity. Yann LeCun, the chief AI scientist at Mark Zuckerberg’s Meta and a senior figure in the field, has <a href="https://x.com/ylecun/status/1719475457265938604">denied there is an existential threat</a> and said AI “could actually save humanity from extinction”.</p> <p>Soares said it was an “easy call” to state that tech companies would reach super-intelligence, but a “hard call” to say when.</p> <p>“We have a ton of uncertainty. I don’t think I could guarantee we have a year [before ASI is achieved]. I don’t think I would be shocked if we had 12 years,” he said.</p> <p>Zuckerberg, a major corporate investor in AI research, has said developing super-intelligence is now “in sight”.</p> <p>“These companies are racing for super-intelligence. That’s their reason for being,” said Soares.</p> <p>“The point is that there’s all these little differences between what you asked for and what you got, and people can’t keep it directly on target, and as an AI gets smarter, it being slightly off target becomes a bigger and bigger deal.”</p> <p>Soares said one policy solution to the threat of ASI was for governments to adopt a multilateral approach echoing the UN treaty on <a href="https://treaties.unoda.org/t/npt">non-proliferation of nuclear weapons</a>.</p> <p>“What the world needs to make it here is a global de-escalation of the race towards super-intelligence, a global ban of … advancements towards super-intelligence,” he said.</p> <aside class="element element-rich-link element--thumbnail"> <p> <span>Related: </span><a href="https://www.theguardian.com/technology/2025/sep/05/anthropic-settlement-ai-book-lawsuit">AI startup Anthropic agrees to pay $1.5bn to settle book piracy lawsuit</a> </p> </aside>  <p>Last month, Raine’s family <a href="https://www.theguardian.com/technology/2025/aug/27/chatgpt-scrutiny-family-teen-killed-himself-sue-open-ai">launched legal action against the owner of ChatGPT, OpenAI</a>. Raine took his own life in April after what his family’s lawyer called “months of encouragement from ChatGPT”. OpenAI, which extended its “deepest sympathies” to Raine’s family, is now implementing guardrails around “sensitive content and risky behaviours” for under-18s.</p> <p><a href="https://www.theguardian.com/society/2025/aug/30/therapists-warn-ai-chatbots-mental-health-support">Psychotherapists have also said</a> that vulnerable people turning to AI chatbots instead of professional therapists for help with their mental health could be “sliding into a dangerous abyss”. Professional warnings of the potential for harm include a <a href="https://osf.io/preprints/psyarxiv/cmy7n_v3">preprint academic study</a> published in July, which reported that AI may amplify delusional or grandiose content in interactions with users vulnerable to psychosis.</p> <p>•<em> In the US, you can call or text the <a href="https://988lifeline.org/">National Suicide Prevention Lifeline</a> on 988, chat on <a href="https://988lifeline.org/chat/">988lifeline.org</a>, or <a href="https://www.crisistextline.org/">text HOME</a> to 741741 to connect with a crisis counselor. In the UK and Ireland, <a href="https://www.samaritans.org/">Samaritans</a> can be contacted on freephone 116 123, or email <a href="mailto:jo@samaritans.org">jo@samaritans.org</a> or <a href="mailto:jo@samaritans.ie">jo@samaritans.ie</a>. In Australia, the crisis support service <a href="https://www.lifeline.org.au/">Lifeline</a> is 13 11 14. Other international helplines can be found at <a href="http://www.befrienders.org/">befrienders.org</a></em></p>