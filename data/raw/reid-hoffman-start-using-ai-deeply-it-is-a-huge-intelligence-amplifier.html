<h1>Reid Hoffman: ‘Start using AI deeply. It is a huge intelligence amplifier’</h1>
<div><strong>Date :</strong> 2025-03-22T18:00:00Z &nbsp; | &nbsp; <strong>Auteur :</strong> Zoë Corbyn &nbsp; | &nbsp; <strong>Journal :</strong> Technology</div>
<p>Reid Hoffman is a prominent Silicon Valley billionaire entrepreneur and investor known for co-founding the professional social networking site LinkedIn, now owned by Microsoft. He’s also staunchly anti-Trump. The longtime Democrat donor threw his support behind Kamala Harris in the race for the White House. Hoffman spoke to the <em>Observer</em> about technology in the new political milieu and his new book about our future with artificial intelligence, <em>Superagency</em>. The book, while not ignoring the problems that AI might cause<em>, </em>argues that the technology is poised to give us cognitive superpowers that will increase our individual and collective human agency, creating a state of widespread empowerment for society.</p> <p><strong>You have a vested interest in being positive about AI, including a company focused on conversational AI for business, </strong><strong>Inflection AI.</strong><strong> Why should we listen to you? <br></strong>First, an economic interest doesn’t necessarily make what someone is saying wrong, and I try to be transparent and not hide mine. Second, I tend to start with my beliefs and follow with my money. And sometimes that does mean doing things that are against my economic interests. Not kissing [Trump’s] ring, like many others have, is probably an economic limiter – but it’s better to be principled. I could have put the time and energy I spent writing <em>Superagency</em> into my companies and made much more money, but I want to share in the intellectual discourse.</p> <p><strong>What’s your hope for the book? <br></strong>I want to at the very least make people AI-curious, so they begin to explore what these superpowers we all might be getting are. There’s a flood of discussion around AI that tends to be negative and concerns a decrease in human agency. And, while that is a common response to new technologies, in previous cases it hasn’t come to pass – human agency has increased – and I predict the AI revolution will land in the same place. But there is a turbulent transition. I am calling it the “cognitive Industrial Revolution” not only because of the anticipated superpowers and superagency we will gain, but because, like the Industrial Revolution, the transition will be difficult. We can get through it with less pain and more grace, as well as improve the state that we’re moving towards, if we use a techno-humanist compass that points us towards building technologies that increase human agency.</p>  <aside class="element element-pullquote element--supporting"> <blockquote> <p>I do believe AI will primarily still end up being a co-pilot, though obviously some job types will disappear</p> </blockquote> </aside>  <p><strong>You argue that AI chatbots such as </strong><strong>ChatGPT</strong><strong> have been a turning point in increasing human agency because, compared </strong><strong>with AI technologies such as facial recognition, predictive policing and algorithmic surveillance, they work </strong><em><strong>for </strong></em><strong>us and </strong><em><strong>with </strong></em><strong>us rather than </strong><em><strong>on</strong></em><strong> us and we must affirmatively choose to use them. But they can still guide us towards certain viewpoints, numb our critical thinking and, of course, potentially upend our jobs </strong>–<strong> which would seem to undermine human agency. <br></strong>To take jobs, they are being transformed, and<strong> </strong>information professionals will need to be using AI tools to do part of their work. If you don’t you will be under-tooled and won’t be competitive. And you may feel that as a loss of agency. You don’t want to change but you can’t choose not to. But then you begin to see the upside: repetitive tasks might be automated, creative processes accelerated. You get a lot more agency and so do other people.</p> <p><strong>So we won’t all become obsolete? <br></strong>I do believe AI will primarily still end up being a co-pilot, though obviously some job types will disappear. We will need to build technologies that can help people whose jobs are changing adapt or, if the job does completely goes away, find other jobs that they can learn and possibly do with AI.</p> <p><strong>You label those who fret about AI’s near-term risks and harms as “gloomers” – but isn’t it important to be critical of new technology? <br></strong>Yes, but where the implication is we should stop or massively slow down – it isn’t helpful. Not least because the countries that adopt the cognitive Industrial Revolution early and firmly will win massive amounts of economic strength, and their values will end up shaping the world. I want western democracies to get there ahead of others like China that are trying to embrace it through autocracy.</p> <p>You get to the good future by steering towards it. Not that you don’t pay attention to the bad futures – but you do so as you’re figuring out how to navigate the right way. You adopt an iterative deployment stance – testing deliberately incremental versions en masse in the real world and then looking at where the critiques play and adjusting (which is how OpenAI has rolled out ChatGPT).</p> <p><strong>Where’s the leadership among wealthy </strong><strong>Democrats against Trump? Or are people lying low fearing political retribution, which <a href="https://www.thetimes.com/business-money/companies/article/linkedin-founder-reid-hoffman-ive-had-threats-of-violence-since-musk-slanders-nfclq2s0r">you have said</a> you are worried about? <br></strong>Personally, I am regrouping. The point, for me, isn’t fighting Trump. It is helping humanity and making society better, including American society. And you might say that because this administration is not going to listen to my ideas about what the government should be doing on AI, maybe I should focus on contributing elsewhere. I recently launched Manas AI, centred on discovering drugs to cure cancer, and I also recently became a fellow of the London School of Economics, helping them think about how AI reinvents the university.</p> <p>That said, obviously I’ve been dismayed and deeply concerned about various things that have happened since Trump took office, including seemingly allying with Russia and Putin and standing down our offensive cyber operations.</p> <p><strong>You are among the</strong> <strong>few tech moguls who haven’t co</strong><strong>sied up to President Trump</strong><strong>. What should we conclude about the morality of this industry </strong><strong>as it rolls back DEI (diversity) initiatives and drops factchecking</strong><strong>, as Meta has done? <br></strong>I do have some quieter friends! The technology industry should be in dialogue and taking some of its cues from a government elected by democratic vote. The fact that you don’t happen to like this government doesn’t negate that. But, on the other hand, there are things being done that are, frankly, bad for society. While one could easily argue some DEI initiatives have gone too far and it’s good to adjust them, some of DEI is civil rights – which, unequivocally, it is good to be for.</p> <p>I obviously disagree with some of the moves that have been made to remove factchecking. We have anti-vax claims in various social networking platforms that are pretty easily false and a double-digit percentage of Americans who believe various vaccine-related conspiracy theories. That kind of level of disinformation within society makes it hard for democracies to operate. LinkedIn gets criticised as boring – but it exemplifies a lot of what I think should be happening on social networks around factchecking.</p>    <p><strong>How worrying has it been to watch the “move fast and break things” tech approach being applied to the US federal government by Doge</strong><strong> [Elon Musk’s </strong><strong>department of government effeciency], in some cases reportedly using AI software to identify budget cuts? <br></strong>I expect most business people, including myself, would think that figuring out how to make government more efficient is a good goal. But you can do it in a way that’s less destructive, less cruel and more lawful than is being done. Firing all these experts and then trying to rehire them: it is a hot mess of incompetent behaviour. Even doing it hard and fast, there are ways – they could have asked for memos about the programmes they were thinking of cutting. The “I’ll just cancel all of it and then we’ll see what happens” is a path that comes with massive external costs.</p> <p><strong>You and Elon Musk used to be friends</strong><strong>. But he has accused you and continues to repeat the accusation that you were one of Jeffrey Epstein’s clients – which you have said is “slander and a lie” and that your only involvement with Epstein, which you have apologised for, was in helping the MIT Media Lab fundraise.</strong> <strong>Any plans to take legal action? <br></strong>I have yet to ever personally bring a lawsuit. I tend to be a builder and a maker and legal action of this sort is very difficult in the US. I have thought about also calling for the Epstein files to be released to bring out the truth. But then do I really want to get into that tar pit? I question Elon’s motives for saying these things given now he’s in government. I think he is trying to smear me to try to make my voice less relevant to the American people.</p> <p><strong>How do you AI-proof yourself? What’s your advice to a young adult thinking about a career path? <br></strong>I don’t think it is AI-proofing yourself; it is AI-amplifying yourself. The important thing is to be engaging with AI and learning the tools. And young people have a real advantage: because they are interested in and tend to adopt new technologies readily, they can bring a skill set and mindset to the workplace that can help the workplace transform.</p> <p><strong>Your previous book, </strong><em><strong>Impromptu</strong></em><strong>, is described as written “</strong><strong>by Reid Hoffman</strong><strong> with ChatGPT-4</strong><strong>” and it documents your conversations with the chatbot. To what extent did you use AI to write this book? <br></strong>While [my co-author and I] feel we own every word here, we used it a lot! For research, to give us the pros and cons of what we were arguing in various sections, and to suggest rewrites of paragraphs to give them more zing. My recommendation to all writers is to start using AI deeply. It is a huge intelligence amplifier. And I don’t think it is necessary the way we used it – which wasn’t wholesale quoting – to say “written with AI”. It is would be like saying “written with a Mac”.</p> <p><strong>How should we regulate AI? Biden’s 2023 executive order, which was the closest the US had to any federal AI regulation and was aimed at trying to reduce its risks, has been rescinded by Trump, who described it as a barrier to American AI innovation. <br></strong>Regulation should be iterative – like deployment. So certainly, regulation as we go and even some regulation now. The Biden executive order was directionally right, meant as it was to tackle major harms rather than any harm you could think of, and hopefully it is just cancelled in name. But it isn’t just regulation. Feedback from customers, employees, the public are all part of steering the path here. And benchmarks and metrics can be important too as a non-legally binding way of shelving algorithms that perform poorly.</p> <p><strong>Are we going to end up with chatbots built on partisan large language models (LLMs) that steer clear of truth and reinforce our own worldviews? <br></strong>Obviously, it’s not good for us to be in complete filter bubbles. And I think you will get some of that with some LLMs. I am a fan of identifying the principles you are training your LLM on and then articulating a reasoned argument for that. So: I’m the “anti-woke” LLM because I believe this, this and this, or I’m an LLM that will let you know when there’s a big swath of people who disagree with you because I think it is important for you to be informed. Then people will know what they are using.</p> <p><strong>Is technologists’ holy grail of reaching artificial general intelligence (AGI) – in which AI can perform any intellectual task that a human can and which many expect will be achieved before the end of the decade – needed for the cognitive </strong><strong>Industrial Revolution?</strong><strong> <br></strong>Not necessarily, although it would be an even further amplification. LLMs today are enabling us to do things that no human being can do, in terms of the breadth of knowledge, and bringing things together. I do think that within three years the tools will be good enough that, if you’re not using them, it will be like a professional not having a mobile phone. But whether we are talking about AGI or even artificial super intelligence (ASI) [greatly exceeding human cognitive abilities], and which I think is at least decades away, we should try to shape them in a way that’s good for us and good for society. Let’s make sure ASIs are essentially Buddhist in their values.</p> <ul> <li><p><em>Superagency: What Could Possibly Go Right With Our AI Future?</em> by Reid Hoffman and Greg Beato is published by Authors Equity (£22). To support the <em>Guardian</em> and <em>Observer</em> order your copy at <a href="https://guardianbookshop.com/superagency-9798893310108/?utm_source=editoriallink&amp;utm_medium=merch&amp;utm_campaign=article">guardianbookshop.com</a>. Delivery charges may apply </p></li> </ul>