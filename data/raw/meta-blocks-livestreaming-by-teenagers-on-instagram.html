<h1>Meta blocks livestreaming by teenagers on Instagram</h1>
<div><strong>Date :</strong> 2025-04-08T12:07:08Z &nbsp; | &nbsp; <strong>Auteur :</strong> Dan Milmo Global technology editor &nbsp; | &nbsp; <strong>Journal :</strong> Technology</div>
<p>Meta is expanding its safety measures for teenagers on Instagram with a block on livestreaming, as the social media company <a href="https://www.theguardian.com/technology/2024/sep/17/meta-instagram-facebook-teen-accounts-social-media-ban-australia">extends its under-18 safeguards</a> to the Facebook and Messenger platforms.</p> <p>Under-16s will be barred from using Instagram’s Live feature unless they have parental permission. They will also require parental permission to turn off a feature that blurs images containing suspected nudity in their direct messages.</p> <p>The changes were announced alongside the extension of Instagram’s teen accounts system to Facebook and Messenger. Teen accounts were introduced last year and placed under-18s by default into a setting that includes giving parents the ability to set daily time limits for using the app, to block teenagers from using Instagram at certain times and to see the accounts with which their child is exchanging messages.</p> <p>Facebook and Messenger teen accounts will be rolled out initially in the US, UK, Australia and Canada. As with the Instagram accounts, users under the age of 16 will need parental permission to change the settings, while 16 and 17-year-olds defaulted into the new features will be able to change them independently.</p> <p>Meta said the Instagram teen accounts were used by 54 million under-18s around the world, with more than 90% of 13- to 15-year-olds keeping on their default restrictions.</p> <p>The NSPCC, a leading child protection charity, said it welcomed extending the measures to Facebook and Messenger, but said Meta had to do more work to prevent harmful material appearing on its platforms.</p> <p>“For these changes to be truly effective, they must be combined with proactive measures so dangerous content doesn’t proliferate on Instagram, Facebook and Messenger in the first place,” said Matthew Sowemimo, the associate head of policy for child safety online at the NSPCC.</p> <p>The announcement was made as the UK implements the Online Safety Act. Since March, every site and app within the scope of the <a href="https://www.theguardian.com/law/article/2024/aug/08/what-is-uk-online-safety-act-new-legislation-laws">legislation</a>, which covers more than 100,000 services from Facebook, Google and X to Reddit and OnlyFans, is required to take steps to stop the appearance of illegal content such as child sexual abuse, fraud and terrorism material, or to take it down if it goes online.</p> <p>The act also contains provisions for protecting children from harm and requires tech platforms to shield under-18s from damaging material such as suicide and self-harm-related content. Reports last week that the act could be watered down as part of a UK-US trade deal were met with protests from child safety groups, which said any compromise would be an <a href="https://www.theguardian.com/technology/2025/apr/03/dont-weaken-online-safety-laws-children-uk-us-trade-deal">“appalling sellout”</a> that would be rejected by voters.</p> <p>Speaking at the time the Instagram restrictions were launched, Meta’s then president of global affairs, Nick Clegg, said the aim was to “shift the balance in favour of parents” when it came to using parental controls. The announcement had come days after Clegg said parents <a href="https://www.theguardian.com/technology/2024/sep/12/parental-controls-facebook-instagram-meta-nick-clegg">tended not to use child safety measures</a>.</p>