<h1>Social media incentivised spread of Southport misinformation, MPs say</h1>
<div><strong>Date :</strong> 2025-07-10T23:01:50Z &nbsp; | &nbsp; <strong>Auteur :</strong> Robert Booth UK technology editor &nbsp; | &nbsp; <strong>Journal :</strong> Media</div>
<p>Social media business models endangered the public by incentivising the spread of dangerous misinformation after the 2024 Southport murders, MPs have concluded, adding that current online safety laws have “major holes”.</p> <p>The Commons science and technology select committee called for new multimillion-pound fines for platforms that do not set out how they will tackle the spread of harmful content through their recommendation systems.</p> <p>The MPs warned that rapid advances in generative artificial intelligence, which allows for the creation of convincing fake videos, could make the next misinformation crisis “even more dangerous” than last August’s violent protests after three children were killed by a man wrongly identified online as an asylum seeker who had arrived by small boat.</p> <p>They also called for AI-generated content to be visibly labelled and said divisive and deceptive content amplified on social media after the attacks may have been part of a foreign disinformation operation.</p> <p>“It’s clear that the Online Safety Act [OSA] just isn’t up to scratch,” said Chi Onwurah, the committee chair, after a seven-month inquiry. “The government needs to go further to tackle the pervasive spread of misinformation that causes harm but doesn’t cross the line into illegality. Social media companies are not just neutral platforms but actively curate what you see online, and they must be held accountable.”</p> <p>Neither misinformation nor disinformation are harms that firms need to address under the OSA, which only received royal assent less than two years ago. State-sponsored disinformation can amount to an offence of foreign interference.</p> <p>The report examines the role of platforms including X, Facebook and TikTok, and comes after this week’s opening of a public inquiry into missed opportunities to prevent the killing of Bebe King, six, Elsie Dot Stancombe, seven, and Alice da Silva Aguiar, nine, on 29 July last year.</p> <p>Just over two hours after the first call to the emergency services, a post on X claimed the suspect was a “Muslim immigrant”, and within five hours a false name, “Ali al-Shakati”, was circulating on the same platform, the MPs found. Within a day, these two posts had received more than 5m views. In fact, the attacker was Axel Rudakubana, a British citizen born in Cardiff.</p> <p>Another X post that evening calling for violence towards asylum hostels received more than 300,000 views, and the next day the false name was on X’s “Trending in the UK” list.</p> <p>TikTok suggested to users under its “others searched for” function the words “Ali al-Shakti arrested in Southport”, and by the end of the day after the attack social media posts with the false name had accrued 27m impressions and violence had broken out outside Southport mosque. On 3 and 4 August a Facebook post called for violence against the Britannia hotel in Leeds, where many occupants were asylum seekers.</p> <p>The committee called for fines of at least £18m if platforms do not set out how they will tackle significant harms that derive from content promoted by their recommendation systems even if it is not illegal.</p> <p>It concluded: “The act fails to keep UK citizens safe from a core and pervasive online harm.”</p> <p>It called on the government to make social media platforms “identify and algorithmically deprioritise factchecked misleading content, or content that cites unreliable sources, where it has the potential to cause significant harm.” But it stressed: “It is vital that these measures do not censor legal free expression.”</p> <p>The MPs called on ministers to extend regulatory powers to tackle social media advertising systems that allow “the monetisation of harmful and misleading content”, with penalties rising depending on severity and the proceeds used to support victims of online harms.</p> <p>The Department for Science, Innovation and Technology has been approached for comment.</p> <p>Ofcom said it held platforms to account over illegal content but stressed that the scope of laws requiring platforms to tackle legal but harmful content was a matter for the government and parliament.</p> <p>A spokesperson said: “Technology and online harms are constantly evolving, so we’re always looking for ways to make life online safer. We’re proposing stronger protections including asking platforms to do more on recommender systems and to have clear protocols for responding to surges in illegal content during crises.”</p> <p>TikTok said its community guidelines prohibited inaccurate, misleading or false content that may cause significant harm and it worked with factcheckers and made any content that could not be verified as accurate ineligible for its “for you” feed.</p> <p>X and Meta were approached for comment. </p>