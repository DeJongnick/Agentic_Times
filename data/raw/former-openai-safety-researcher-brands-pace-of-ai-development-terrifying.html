<h1>Former OpenAI safety researcher brands pace of AI development ‘terrifying’</h1>
<div><strong>Date :</strong> 2025-01-28T19:06:33Z &nbsp; | &nbsp; <strong>Auteur :</strong> Dan Milmo Global technology editor &nbsp; | &nbsp; <strong>Journal :</strong> Technology</div>
<p>A former safety researcher at OpenAI says he is “pretty terrified” about the pace of development in artificial intelligence, warning the industry is taking a “very risky gamble” on the technology.</p> <p>Steven Adler expressed concerns about companies seeking to rapidly develop artificial general intelligence (AGI), a theoretical term referring to systems that match or exceed humans at any intellectual task.</p> <p>Adler, who left OpenAI in November, said in a series of posts on X that <a href="https://x.com/sjgadler/status/1883928200029602236">he’d had a “wild ride”</a> at the US company and would miss “many parts of it”.</p> <p>However, he said the technology was developing so quickly it <a href="https://x.com/sjgadler/status/1883928201891831828">raised doubts</a> about the <a href="https://www.theguardian.com/technology/2023/may/30/risk-of-extinction-by-ai-should-be-global-priority-say-tech-experts">future of humanity</a>.</p> <p>“I’m pretty terrified by the pace of AI development these days,” he said. “When I think about where I’ll raise a future family, or how much to save for retirement, I can’t help but wonder: will humanity even make it to that point?”</p> <p>Some experts, <a href="https://www.theguardian.com/technology/2024/dec/27/godfather-of-ai-raises-odds-of-the-technology-wiping-out-humanity-over-next-30-years">such as Nobel prize winner Geoffrey Hinton</a>, fear that powerful AI systems could evade human control with potentially catastrophic consequences. Others, such as Meta’s chief AI scientist, Yann LeCun, have <a href="https://x.com/ylecun/status/1719475457265938604">played down the existential threat</a>, saying AI “could actually save humanity from extinction”.</p> <p>According to Adler’s <a href="https://www.linkedin.com/in/sjgadler/">LinkedIn profile</a>, he led safety-related research for “first-time product launches” and “more speculative long-term AI systems” in a four-year career at OpenAI.</p> <p>Referring to the development of AGI, OpenAI’s core goal, Adler added: “An AGI race is a very risky gamble, with huge downside.” Adler said no research lab had a solution to AI alignment – the process of ensuring that systems adhere to a set of human values – and that the industry might be moving too fast to find one.</p> <p>“The faster we race, the less likely that anyone finds one in time.”</p> <p>Adler’s X posts came as China’s DeepSeek, which is also seeking to develop AGI, <a href="https://www.theguardian.com/business/2025/jan/27/tech-shares-asia-europe-fall-china-ai-deepseek">rattled the US tech industry</a> by unveiling a model that rivalled OpenAI’s technology despite being developed with apparently fewer resources.</p> <p>Warning that the industry appeared to be “stuck in a really bad equilibrium”, Adler said “real safety regs” were needed.</p> <p>“Even if a lab truly wants to develop AGI responsibly, others can still cut corners to catch up, maybe disastrously.”</p> <p>Adler and OpenAI have been contacted for comment.</p>