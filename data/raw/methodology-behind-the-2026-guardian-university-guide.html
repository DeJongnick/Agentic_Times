<h1>Methodology behind the 2026 Guardian University Guide</h1>
<div><strong>Date :</strong> 2025-09-13T07:01:03Z &nbsp; | &nbsp; <strong>Auteur :</strong> Matt Hiely-Rayner &nbsp; | &nbsp; <strong>Journal :</strong> Education</div>
<p>We use eight measures of performance, covering all stages of the student life cycle, to put together a league table for 66 subjects. We regard each provider of a subject as a department and ask each provider to tell us which of their students count within each department.</p> <p>Our intention is to indicate how likely each department is to deliver a positive all-round experience to future students and, in order to assess this, we refer to how past students in the department have fared. We quantify the resources and staff contact that have been dedicated to past students, we look at the standards of entry and the likelihood that students will be supported to continue their studies, before looking at how likely students are to be satisfied, to exceed expectations of success and to have positive career prospects after completing the course. Bringing these measures together, we get an overall score for each department and rank departments against this.</p> <p>For comparability, the data we use focuses on full-time first-degree students. For those prospective undergraduates who have not decided which subject they wish to study, but who still want to know where institutions rank in relation to one another, the Guardian scores have been averaged for each institution across all subjects to generate an institution-level table.</p> <h2>Changes in 2026</h2> <p>The structure and methodology of the rankings has remained broadly constant since 2008, though the data that is commissioned by the JISC and HESA sector bodies has been disrupted in recent years.</p> <p><strong>Years from which data is drawn<br></strong>Following the 2025 edition, which suffered from poor data availability in reference to the 2022/23 academic year, the 2026 edition is largely back on track and mainly refers to data from 2023/24.</p> <p><strong>Student-staff ratios and expenditure per student<br></strong>Data from 2023/24 was used to support these metrics, which depend upon a student FTE (full-time equivalent) value in order to express the figures as rates. Staff FTE and rates of expenditure were available in accordance with the definitions of previous years, but the foundations of the student FTE values have changed significantly and needed to be commissioned with new definitions. This involved seeking and distributing a secondary source of data on postgraduate research students and also discounting student FTEs according to whether they are on a full-year placement or year abroad (20% of full FTE) or on a partial year activity of a similar nature (60%).</p> <p><strong>Career prospects<br></strong>The Career prospects score is driven by the annual Graduate outcomes survey, which traces graduate occupations 15 months after they complete their course. Data for the 2021/22 and 2022/23 cohorts was averaged and used for this metric.</p> <p><strong>National Student Survey<br></strong>Data from the 2025 National Student Survey (NSS) was used as the primary source for this year’s metrics on satisfaction with teaching and with feedback. Data from 2024 was used on occasion, for departments that had seen some responses in 2025 but too few to use without aggregation of the previous year’s data.</p> <p><strong>Continuation<br></strong>While other metrics have skipped the troublesome data from 2022/23, the continuation metric was always a year in arrears and, instead of catching up, the data we use has ticked forward a year to refer to first years in 2021/22 and their activities in 2022/23. Limitations in how the records of students are matched between years meant that a feature of the Guardian University Guide’s specification was not adequately supported: students who have progressed into the first year of a degree course after completing a foundation year are supposed to be included, but as non-entrants there were key fields missing that meant their outcomes could not be ascertained. As a consequence, this year exceptionally excludes all students who have studied on a foundation year course from the continuation metric.</p> <p><strong>Other metrics<br></strong>Data on entry standards and value-added scores continue to refer to activity in 2023/24.</p> <p><strong>Franchised provision<br></strong>Some established universities subcontract their courses to be delivered by franchise partners and steps have been taken to review how such activity is counted within the compilation process. In situations where significant volumes of franchised activity (400+ students) outweigh the activity delivered by the registering university we have stripped out the sub-contracted activity from the source data. This ensures that where a university is ranked for a subject, they are delivering at least the majority of activity that is described by the statistics (and in the vast majority of cases, all of it).</p> <h2>Details of each metric</h2> <p><strong>Entry standards<br></strong>This measure seeks to approximate the aptitude of fellow students with whom a prospective student can expect to study and reports the observed average grades of students joining the department – not the conditions of admission to the course that may be advertised. Average tariffs are determined by taking the total tariff points of first-year, first-degree, full-time entrants who were aged under 21 at the start of their course, if the qualifications that they entered with could all be expressed using the tariff system devised by Ucas. There must be more than seven students in any meaningful average and only students entering year one of a course (not a foundation year) with certain types of qualification are included.</p> <p>This metric contributes 15% to the total score of a department (24% for medical subjects) and refers to those who entered the department in 2023/24.</p> <p><strong>Student-staff ratios<br></strong>Student-staff ratios seek to approximate the levels of staff contact that a student can expect to receive by dividing the volume of students who are taking modules in a subject by the volume of staff who are available to teach it. Thus a low ratio is treated positively – it indicates that more staff contact could be anticipated.</p> <p>Staff and students are reported on a “full-time equivalent” basis and research-only staff are excluded from the staff volume. Students on placement, study abroad, or on a course that is franchised to another provider have their volume discounted accordingly.</p> <p>At least 28 students and three staff (both FTE) must be present in an SSR calculation using 2023/24 data alone. Smaller departments that had at least seven student and two staff FTE in 2023/24, and at least 30 student FTE in total across 2021/22 and 2023/24, have a two-year average calculated.</p> <p>This metric contributes 15% to the total score of a department (24% for medical subjects). It is released at Hesa cost centre level, and we map each cost centre to one or more of our subjects.</p> <p><strong>Expenditure per student<br></strong>In order to approximate the level of resources that a student could expect to have dedicated to their provision, we look at the total expenditure in each subject area and divide it by the volume of students taking the subject. We exclude academic staff costs as the benefits of high staff volumes are already captured by the student-staff ratios but recognise that many costs of delivery are centralised: we add the amount of money each provider has spent for students on academic services such as libraries and computing facilities for each student, over the past two years.</p> <p>This metric is expressed as points/10 and contributes 5% to the total score of a department (10% for medical subjects).</p> <p><strong>Continuation<br></strong>Taking a degree-level course is a positive experience for most students but is not suited to everybody and some students struggle and discontinue their studies. Providers can do a lot to support their students – they might promote engagement with studies and with the broader higher education experience or offer dedicated support when students face a obstacle – and this measure captures how successful each department is in achieving this. We look at the proportion of students who continue their studies beyond the first year and measure the extent to which this exceeds expectations based on entry qualifications.</p> <p>To achieve this, we take all first-year students on full-time first-degree courses that are scheduled to take longer than a year to complete and look ahead to 1 December in the following academic year to observe the proportion who are still active in higher education. This proportion is viewed positively, regardless of whether the student has switched course, transferred to a different provider, or been required to repeat their first year – only those who become inactive in the UK’s HE system are counted negatively.</p> <p>To take the effect of entry qualifications into account we create an index score for each student who has a positive outcome, using their expectation of continuation up to a maximum of 97%. To calculate the score there must have been 25 entrants in the most recent cohort and 50 across the last two or three years.</p> <p>This index score, aggregated across the last two or three years, contributes 15% to the total score of non-medical departments and 10% to those of the medical subjects. However, it is the percentage score – also averaged over two or three years – that is displayed.</p> <p>Exceptionally, this year’s edition excludes students who were taking the first year of a degree-level course after completion of a foundation year.</p> <p><strong>Student satisfaction<br></strong>The National Student Survey asks final-year students to respond to questions with varying degrees of positivity, generally with four available responses. For the questions we convert into metrics for the University Guide, we calculate two statistics: a satisfaction rate and an average response. The satisfaction rate captures an average of the responses to the relevant questions, drawing on a spectrum ranging from one for the most negative to four for the most positive.</p> <p>To assess the <strong>teaching quality</strong> a student can expect to experience, we took responses from the 2025 NSS and aggregated them for the following questions:</p> <ul> <li><p>How good are teaching staff at explaining things?</p></li> <li><p>How often do teaching staff make the subject engaging?</p></li> <li><p>How often is the course intellectually stimulating?</p></li> <li><p>How often does your course challenge you to achieve your best work?</p></li> </ul> <p>The satisfaction rate for each provider is displayed, and the average response is used with a 10% weighting (16% for medical subjects).</p> <p>To assess the likelihood that a student will be satisfied with <strong>assessment</strong> procedures and the <strong>feedback</strong> they receive we took responses from the 2025 NSS and aggregated them for the following questions:</p> <ul> <li><p>How clear were the marking criteria used to assess your work?</p></li> <li><p>How fair has the marking and assessment been on your course?</p></li> <li><p>How well have assessments allowed you to demonstrate what you have learned?</p></li> <li><p>How often have you received assessment feedback on time?</p></li> <li><p>How often does feedback help you to improve your work?</p></li> </ul> <p>The overall satisfaction rate for each provider is displayed, and the average response is used with a 10% weighting.</p> <p>Data was released at the CAH (common aggregation hierarchy) levels of aggregation and we used details of how these map to Hecos (Higher education classification of subjects) to weight and aggregate results for each of our 66 subjects, prioritising results from the most granular level. Our aggregation rules required that there were 10 or more respondents for each CAH subject, with results for more general subjects used if this condition was not satisfied. After aggregating these results to the level of Guardian Subject Groups, there needed to be 23 respondents from the 2025 survey or at least 15 from 2025 and a total of 23 across 2024 and 2025 for the resulting statistic to be used in the guide.</p> <p><strong>Value </strong><strong>added<br></strong>In order to assess the extent to which each department will support its students towards achieving good grades, we use value added scores to track students from enrolment to graduation. A student’s chances of getting a good classification of degree (a 1st or a 2:1) are already affected by the qualifications that they start with so our scores take this into account and report the extent to which a student exceeded expectations.</p> <p>Each full-time student is given a probability of achieving a first or 2:1, based on the qualifications that they enter with or, if they have vague entry qualifications, the total percentage of good degrees expected for the student in their department. If they manage to earn a good degree, then they score points that reflect how difficult it was to do so (in fact, they score the reciprocal of the probability of getting a first or 2:1). Otherwise they score zero. Students completing an integrated master’s award are always regarded as having a positive outcome.</p> <p>At least 30 students must be in a subject for a meaningful value added score to be calculated using the most recent year of data alone. If there are more than 15 students in both the most recent year and the preceding year, then a two-year average is calculated.</p> <p>This metric is expressed as points/10 and contributes 15% to the total score of a department but is not used for medical subjects.</p> <p><strong>Career prospects<br></strong>Using results from the Graduate outcomes survey for the graduating cohorts of 2021/22 and 2022/23, we seek to assess the extent to which students have taken a positive first step in the 15 months after graduation, in anticipation that similar patterns will repeat for future cohorts. We value students who enter graduate-level occupations (approximated by SOC groups 1-3: professional, managerial &amp; technical occupations) and students who go on to further study at a professional or HE level and treat these students as positive.</p> <p>Students report one or more activities, and for each of these give more detail. If students are self-employed or working for an employer, we treat them as positive if the occupation is in SOC groups 1-3, if they have either finished a course or are presently taking one then we look at the level and treat them positively accordingly. Students who have no activity that is regarded positively, but who either reported that they were unable to work, or only partially completed the survey leaving details of an activity incomplete, are excluded from the metric.</p> <p>The metric refers only to students who graduated from full-time first-degree courses and we only use results if at least 15 students in a department responded in each of the two years or if at least 22.5 students responded in the most recent year. Partial responses are used if the respondent provided details for any of the activities that they reported undertaking. We exclude the responses if, for an activity, we are unable to determine if it should be treated as a positive outcome.</p> <p>We have always avoided averaging results across years for this metric because the national economic environment that leavers find themselves in can have such a big effect on employment and this is especially true when a pandemic affects the economy. Unfortunately, response rates for the Graduate outcomes survey are not high enough to maintain this stance. We therefore average the career prospects statistics across the two years in an unweighted manner, in order to avoid any advantage or disadvantage for a department that had a higher response for a cohort in which economic conditions were better/worse. In situations where only the most recent year of data meets the threshold for usage we have applied the year-on-year sector difference observed for the subject concerned in order to simulate what a two-year average might have looked like given changing economic conditions.</p> <p>This metric is worth 15% of the total score in all the non-medical subjects.</p> <h2>Using metric results</h2> <p>First of all, we determine if a department has enough data to support a ranking. Often individual metrics are missing and we seek to keep the department in the rankings where we can. An institution can only be included in the table if the weighting value of any indicators that are missing add up to 40% or less, and if the institution’s relevant department teaches at least 35 full-time first-degree students. There must also be at least 25 students (FTE) in the relevant cost centre.</p> <p>For those institutions that qualify for inclusion in the subject table, each score is compared to the average score achieved by the other institutions that qualify, using standard deviations to gain a normal distribution of standardised scores (S-scores). The standardised score for student: staff ratios is negative, to reflect that low ratios are regarded as better. We cap certain S-scores – extremely high NSS, expenditure and SSR figures – at three standard deviations. This is to prevent a valid but extreme value from exerting an influence that far exceeds that of all other measures.</p> <p>For metrics in subjects where there are very few datapoints we refer to the distribution of scores observed for a higher aggregation of subjects (CAH1). We also set a minimum standard deviation for each metric and make adjustments to the mean tariff that is referenced by departments with students who entered with Scottish highers or advanced highers.</p> <p>Although we don’t display anything, we need to plug the gap left in the total score that is left by any missing indicators. We use a substitution process that first looks for the corresponding standardised score in the previous year and then, if nothing is available, resorts to looking at whether the missing metric is correlated to general performance in that subject. If it is, the department’s performance in the other metrics is used – effectively assuming that it would have performed as well in the missing metric as it did in everything else. If not, the average score achieved by other providers of the subject is used.</p> <p>Using the weighting attached to each metric, the standardised scores are weighted and totalled to give an overall departmental score (rescaled to 100) against which the departments are ranked.</p> <h2>The institutional ranking</h2> <p>The institutional table ranks institutions according to their performance in the subject tables but considers two other factors when calculating overall performance.</p> <p>First, the number of students in a department influences the extent to which that department’s total standardised score contributes to the institution’s overall score. And second, the number of institutions included in the subject table determines the extent to which a department can affect the institutional table.</p> <p>The number of full-time undergraduates in each subject is expressed as a percentage of the total number of full-time undergraduates counted in subjects for which the institution is included within the subject table. For each subject, the number of institutions included within the table is counted and the natural logarithm of this value is calculated. The total S-Score for each subject – which can be negative or positive – is multiplied by these two values, and the results are summed for all subjects, to give an overall S-score for each institution. Institutions are ranked according to this overall S-score, though the value displayed in the published table is a scaled version of this, that gives the top university 100 points and all the others a smaller (but positive) points tally.</p> <p>Each institution has overall versions of each of the indicators displayed next to its overall score out of 100, but these are crude institutional averages that are otherwise disconnected from the tables and give no consideration to subject mix. Therefore these institutional averages cannot be used to calculate the overall score or ranking position.</p> <p>The indicators of performance for value added and for expenditure per student are treated slightly differently, because they need to be converted into points out of 10 before being displayed. Therefore these indicators do read from the subject level tables, again using student numbers to create a weighted average.</p> <p>Institutions that appear in fewer than eight subject tables are not included in the main ranking of universities.</p> <h2>Course directory</h2> <p>The KIS database of courses, to which institutions provide regular updates to describe courses that students will be able to apply for in future years, is the data source of the courses that we list under each department in each subject group.</p> <p>We have associated each full-time course with one or more subject groups, based on the subject data associated with the courses. We gave institutions the freedom to adjust these associations with subjects and also to change details of the courses. We include courses that are not at degree level, even though such provision is excluded from the data used to generate scores and rankings.</p>