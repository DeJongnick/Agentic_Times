<h1>Use of AI could worsen racism and sexism in Australia, human rights commissioner warns</h1>
<div><strong>Date :</strong> 2025-08-13T06:05:08Z &nbsp; | &nbsp; <strong>Auteur :</strong> Krishani Dhanji &nbsp; | &nbsp; <strong>Journal :</strong> Technology</div>
<p>AI risks entrenching racism and sexism in Australia, the human rights commissioner has warned, amid internal Labor debate about how to respond to the emerging technology.</p> <p>Lorraine Finlay says the pursuit of productivity gains from AI should not come at the expense of discrimination if the technology is not properly regulated.</p> <p>Finlay’s comments follow Labor senator Michelle Ananda-Rajah breaking ranks to call for all Australian data to be “freed” to tech companies to prevent AI perpetuating overseas biases and reflect Australian life and culture.</p> <p>Ananda-Rajah is opposed to a dedicated AI act but believes content creators should be paid for their work.</p> <p><a href="https://www.theguardian.com/email-newsletters?CMP=copyembed&amp;CMP=emailbutton"><sub>Sign up: AU Breaking News email</sub></a></p> <p>Productivity gains from AI will be discussed next week at the federal government’s economic summit, as unions and industry bodies raise concerns about <a href="https://www.theguardian.com/technology/2025/aug/05/productivity-commission-digital-economy-report-copyright-rules-text-and-data-mining-to-train-ai-models">copyright and privacy protections</a>. </p> <p>Media and arts groups have warned of <a href="https://www.theguardian.com/technology/2025/aug/06/arts-and-media-groups-demand-labor-take-a-stand-against-rampant-theft-of-australian-content-to-train-ai">“rampant theft” of intellectual property</a> if big tech companies can take their content to train AI models.</p> <p>Finlay said a lack of transparency in what datasets AI tools are being trained on makes it difficult to identify which biases it may contain.</p> <p>“Algorithmic bias means that bias and unfairness is built into the tools that we’re using, and so the decisions that result will reflect that bias,” she said.</p>    <p>“When you combine algorithmic bias with automation bias – which is where humans are more likely to rely on the decisions of machines and almost replace their own thinking – there’s a real risk that what we’re actually creating is discrimination and bias in a form where it’s so entrenched, we’re perhaps not even aware that it’s occurring.”</p> <p>The Human Rights Commission has consistently advocated for an AI act, bolstering existing legislation, including the Privacy Act, and rigorous testing for bias in AI tools. Finlay said the government should urgently establish new legislative guardrails.</p> <p>“Bias testing and auditing, ensuring proper human oversight review, you [do] need those variety of different measures in place,” she said.</p> <aside class="element element-rich-link element--thumbnail"> <p> <span>Related: </span><a href="https://www.theguardian.com/australia-news/2025/aug/13/labor-federal-election-review-emerging-misinformation-and-ai-threats">AI and misinformation in crosshairs of Labor’s review of its landslide election win</a> </p> </aside>  <p>There is growing evidence that there is bias in AI tools in Australia and <a href="https://www.theguardian.com/technology/2025/aug/11/ai-tools-used-by-english-councils-downplay-womens-health-issues-study-finds">overseas</a>, in areas such as medicine and job recruitment. </p> <p><a href="https://www.theguardian.com/australia-news/2025/may/14/people-interviewed-by-ai-for-jobs-face-discrimination-risks-australian-study-warns#:~:text=People%20interviewed%20by%20AI%20for%20jobs%20face%20discrimination%20risks%2C%20Australian%20study%20warns,-This%20article%20is&amp;text=Job%20candidates%20being%20interviewed%20by,a%20new%20study%20has%20warned.">An Australian study</a> published in May found job candidates being interviewed by AI recruiters risked being discriminated against if they spoke with an accent or were living with a disability.</p> <p>Ananda-Rajah,<strong> </strong>who was<strong> </strong>a medical doctor and researcher in AI before entering parliament, said it was important for AI tools to be trained on Australian data, or risk perpetuating overseas biases.</p> <p>While the government has stressed the need for protecting intellectual property, she warned that not opening up domestic data would mean Australia would be “forever renting [AI] models from tech behemoths overseas” with no oversight or insight into their models or platforms.</p> <p>“AI must be trained on as much data as possible from as wide a population as possible or it will amplify biases, potentially harming the very people it is meant to serve,” Ananda-Rajah said.</p> <p>“We need to free our own data in order to train the models so that they better represent us.</p> <p>“I’m keen to monetise content creators while freeing the data. I think we can present an alternative to the pillage and plunder of overseas.”</p> <p>Ananda-Rajah raised <a href="https://www.theguardian.com/society/2021/nov/09/ai-skin-cancer-diagnoses-risk-being-less-accurate-for-dark-skin-study">skin cancer screening</a> by AI as an example where the tools used for testing have been shown to have algorithmic bias. Ananda-Rajah said the way to overcome any bias or discrimination against certain patients would be to train “these models on as much diverse data from Australia as possible”, with appropriate protections for sensitive data.</p> <aside class="element element-rich-link element--thumbnail"> <p> <span>Related: </span><a href="https://www.theguardian.com/commentisfree/2025/aug/13/can-ai-deliver-economic-nirvana-only-if-workers-can-monitor-and-shape-how-its-used">Can AI deliver economic nirvana? Only if workers can monitor and shape how it’s used | Peter Lewis</a> </p> </aside>  <p>Finlay said any release of Australian data should be done in a fair way but she believes the focus should be on regulation.</p> <p>“Having diverse and representative data is absolutely a good thing … but it’s only one part of the solution,” she said.</p> <p>“We need to make sure that this technology is put in place in a way that’s fair to everybody and actually recognises the work and the contributions that humans are making.”</p> <p> An AI expert at La Trobe university and former data researcher at an AI company, Judith Bishop, said freeing up more Australian data could help train AI tools more appropriately – while warning AI tools developed overseas using international data may not reflect the needs of Australians – but that it was a small part of the solution.</p> <p>“We have to be careful that a system that was initially developed in other contexts is actually applicable for the [Australian] population, that we’re not relying on US models which have been trained on US data,” Bishop said.</p> <p>The eSafety commissioner, Julie Inman Grant, is also concerned by the lack of transparency around the data AI tools use.</p> <p>In a statement, she said tech companies should be transparent about their training data, develop reporting tools and must use diverse, accurate and representative data in their products.</p> <p>“The opacity of generative AI development and deployment is deeply problematic,” Inman Grant said. “This raises important questions about the extent to which LLMs [large language models] could amplify, even accelerate, harmful biases – including narrow or harmful gender norms and racial prejudices. </p> <p>“With the development of these systems concentrated in the hands of a few companies, there’s a real risk that certain bodies of evidence, voices and perspectives could be overshadowed or sidelined in generative outputs.”</p>  <figure class="element element-embed" data-alt="Breaking News">  <iframe class="fenced" srcdoc="&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;script src=&quot;https://uploads.guim.co.uk/2025/01/21/article-button.js&quot;&gt;&lt;script&gt;&lt;/body&gt;&lt;/html&gt;"></iframe> </figure>