<h1>AI can spontaneously develop human-like communication, study finds</h1>
<div><strong>Date :</strong> 2025-05-14T18:00:03Z &nbsp; | &nbsp; <strong>Auteur :</strong> Raphael Boyd &nbsp; | &nbsp; <strong>Journal :</strong> Technology</div>
<p>Artificial intelligence can spontaneously develop human-like social conventions, a study has found.</p> <p>The <a href="https://www.science.org/doi/10.1126/sciadv.adu9368">research</a>, undertaken in collaboration between City St George’s, University of London and the IT University of Copenhagen, suggests that when large language model (LLM) AI agents such as <a href="https://www.theguardian.com/technology/chatgpt">ChatGPT</a> communicate in groups without outside involvement they can begin to adopt linguistic forms and social norms the same way that humans do when they socialise.</p> <p>The study’s lead author, Ariel Flint Ashery, a doctoral researcher at City St George’s, said the group’s work went against the majority of research done into AI, as it treated AI as a social rather than solitary entity.</p> <p>“Most research so far has treated LLMs in isolation but real-world AI systems will increasingly involve many interacting agents,” said Ashery.</p> <p>“We wanted to know: can these models coordinate their behaviour by forming conventions, the building blocks of a society? The answer is yes, and what they do together can’t be reduced to what they do alone.”</p> <p>Groups of individual LLM agents used in the study ranged from 24 to 100 and, in each experiment, two LLM agents were randomly paired and asked to select a “name”, be it a letter or string of characters, from a pool of options.</p> <p>When both the agents selected the same name they were rewarded, but when they selected different options they were penalised and shown each other’s choices.</p> <aside class="element element-rich-link element--supporting"> <p> <span>Related: </span><a href="https://www.theguardian.com/technology/ng-interactive/2025/may/13/chatgpt-ai-big-tech-cooperation">ChatGPT may be polite, but it’s not cooperating with you</a> </p> </aside>  <p>Despite agents not being aware that they were part of a larger group and having their memories limited to only their own recent interactions, a shared naming convention spontaneously emerged across the population without a predefined solution, mimicking the communication norms of human culture.</p> <p>Andrea Baronchelli, a professor of complexity science at City St George’s and the senior author of the study, compared the spread of behaviour with the creation of new words and terms in our society.</p> <p>“The agents are not copying a leader,” he said. “They are all actively trying to coordinate, and always in pairs. Each interaction is a one-on-one attempt to agree on a label, without any global view.</p> <p>“It’s like the term ‘spam’. No one formally defined it, but through repeated coordination efforts, it became the universal label for unwanted email.”</p> <p>Additionally, the team observed collective biases forming naturally that could not be traced back to individual agents.</p> <p>In a final experiment, small groups of AI agents were able to steer the larger group towards a new naming convention.</p> <p>This was pointed to as evidence of critical mass dynamics, where a small but determined minority can trigger a rapid shift in group behaviour once they reach a certain size, as found in human society.</p> <p>Baronchelli said he believed the study “opens a new horizon for AI safety research. It shows the depth of the implications of this new species of agents that have begun to interact with us and will co-shape our future.”</p> <p>He added: “Understanding how they operate is key to leading our coexistence with AI, rather than being subject to it. We are entering a world where AI does not just talk – it negotiates, aligns and sometimes disagrees over shared behaviours, just like us.”</p> <p>The peer-reviewed study, Emergent Social Conventions and Collective Bias in LLM Populations, is published in the journal Science Advances.</p>