<h1>Policymakers who think AI can help rescue flagging UK economy should take heed</h1>
<div><strong>Date :</strong> 2025-06-15T11:00:39Z &nbsp; | &nbsp; <strong>Auteur :</strong> Heather Stewart &nbsp; | &nbsp; <strong>Journal :</strong> Business</div>
<p>From helping consultants <a href="https://www.bbc.co.uk/news/articles/c04e6vxkky5o">diagnose cancer</a>, to aiding teachers in drawing up lesson plans – and <a href="https://reutersinstitute.politics.ox.ac.uk/news/ai-generated-slop-quietly-conquering-internet-it-threat-journalism-or-problem-will-fix-itself">flooding social media with derivative slop</a> – generative artificial intelligence is being adopted across the economy at breakneck speed.</p> <p>Yet a growing number of voices are starting to ask how much of an asset the technology can be to the UK’s sluggish economy. Not least because there is no escaping a persistent flaw: large language models (LLMs) remain prone to casually making things up.</p> <p>It’s a phenomenon known as “hallucination”. In a <a href="https://thebarristergroup.co.uk/blog/responding-to-ai-hallucinations-in-uk-jurisprudence">recent blogpost</a>, the barrister Tahir Khan cited three cases in which lawyers had used large language models to formulate legal filings or arguments – only to find they slipped in fictitious supreme court cases, and made up regulations, or nonexistent laws.</p> <aside class="element element-rich-link element--thumbnail"> <p> <span>Related: </span><a href="https://www.theguardian.com/technology/2025/jun/15/government-roll-out-humphrey-ai-tool-reliance-big-tech">UK government rollout of Humphrey AI tool raises fears about reliance on big tech</a> </p> </aside>  <p>“Hallucinated legal texts often appear stylistically legitimate, formatted with citations, statutes, and judicial opinions, creating an illusion of credibility that can mislead even experienced legal professionals,” he warned.</p> <p>In a <a href="https://www.adam-buxton.co.uk/podcasts/rhwkafw2z2x3xe6-6ysb3-pfp9n-6xxa2-jnpn5-2cacw-6cz8f-gx8fb-nkb87-c6mae-bd5zy-yby4r-6392t-jf5tm-ph55s-cktl3-dsw49-r8ccg-sj9we-wzpe5-nzgjl-hn42k-r46sa-bp623-t4w8l-2pdd3-rbfnb-8hlw3-ayw78-yftnb-3dzhx">recent episode of his podcast</a>, the broadcaster Adam Buxton read out excerpts from a book he had bought online, purporting to be a compilation of quotes and anecdotes about his own life, many of which were superficially plausible – but completely fictitious.</p> <p>The tech-sceptic journalist Ed Zitron argued <a href="https://www.wheresyoured.at/wheres-the-money/">in a recent blogpost</a> that the tendency of ChatGPT (and every other chatbot) to “assert something to be true, when it isn’t”, meant it was “a non-starter for most business customers, where (obviously) what you write has to be true”.</p> <p>Academics at the University of Glasgow have said that because the models are not set up to solve problems, or to reason, but to predict the most plausible-sounding sentence based on the reams of data they have hoovered up, a better word for their factual hiccups is not “hallucinations” but “bullshit”.</p> <p>In a paper from last year that glories in the title <a href="https://link.springer.com/article/10.1007/s10676-024-09775-5">“ChatGPT is bullshit”</a>, Michael Townsen Hicks and his colleagues say: “Large language models simply aim to replicate human speech or writing. This means that their primary goal, insofar as they have one, is to produce human-like text. They do so by estimating the likelihood that a particular word will appear next, given the text that has come before.”</p> <p>In other words, the “hallucinations” are not glitches likely to be ironed out – but integral to the models. A <a href="https://www.newscientist.com/article/2479545-ai-hallucinations-are-getting-worse-and-theyre-here-to-stay/">recent paper in New Scientist</a> suggested they are getting more frequent.</p> <p>Even the cutting-edge forms of AI known as “large reasoning models” <a href="https://www.theguardian.com/technology/2025/jun/09/apple-artificial-intelligence-ai-study-collapse">suffer “accuracy collapse”</a> when faced with complex problems, according to a much-shared paper from Apple last week.</p> <p>None of this is to subtract from the usefulness of LLMs for many analytical tasks – and neither are LLMs the full extent of generative AI; but it does make it risky to lean on chatbots as authorities – as those lawyers found.</p> <p>If LLMs really are more bullshitters than reasoning machines, that has several profound implications.</p> <p>First, it raises questions about the extent to which AI should really be replacing – rather than augmenting or assisting – human employees, who take ultimate responsibility for what they produce.</p> <p>Last year’s joint winner of the Nobel prize for economics Daron Acemoglu says that given its issues with accuracy, generative AI as currently conceived will replace only a narrowly defined set of roles in the foreseeable future. “It’s going to impact a bunch of office jobs that are about data summary, visual matching, pattern recognition, etc. And those are essentially about 5% of the economy,” <a href="https://economics.mit.edu/news/daron-acemoglu-what-do-we-know-about-economics-ai">he said in October</a>.</p> <p>He calls for more research effort to be directed towards building AI tools that workers can use, rather than bots aimed at replacing them altogether.</p> <p>If he is right, AI is unlikely to come to the rescue of countries – in particular the UK – whose productivity has never recovered from the global financial crisis and some of whose policymakers are ardently hoping the AI fairy will help workers do more with less.</p> <p>Second, the patchier the benefits of AI, the lower the costs society should be ready to accept, and the more we should be trying to ensure they are borne, and where possible mitigated, by the originators of the models.</p> <p>These include massive energy costs but also the obvious downsides for politics and democracy of flooding the public realm with invented content. As Sandra Wachter of the Oxford Internet Institute put this recently: “Everybody’s just throwing their empty cans into the forest. So it’s going to be much harder to have a nice walk out there because it’s just being polluted, and because those systems can pollute so much quicker than humans could.”</p> <p>Third, governments should rightly be open to adopting new technologies, including AI – but with a clear understanding of what they can and can’t do, alongside a healthy scepticism of some of their proponents’ wilder (and riskier) claims.</p> <p>To ministers’ credit, last week’s spending review talked as much about “digitisation” as about AI as a way of improving public services.</p> <p>Ministers are well aware that long before swathes of civil servants are replaced by chatbots, the UK’s put-upon citizenry would like to be able to hear from their doctor in some other format than a letter.</p> <p>ChatGPT and its rivals have awesome power: they can synthesise vast amounts of information and present it in whatever style and format you choose, and they’re great for unearthing the accumulated wisdom of the web.</p> <p>But as anyone who has met a charming bullshitter in their life will tell you (and who hasn’t?), it is a mistake to think they will solve all your problems – and wise to keep your wits about you.</p>