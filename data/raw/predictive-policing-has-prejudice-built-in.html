<h1>Predictive policing has prejudice built in</h1>
<div><strong>Date :</strong> 2025-04-10T16:56:43Z &nbsp; | &nbsp; <strong>Auteur :</strong> Auteur inconnu &nbsp; | &nbsp; <strong>Journal :</strong> UK news</div>
<p>Re your article (<a href="https://www.theguardian.com/uk-news/2025/apr/08/uk-creating-prediction-tool-to-identify-people-most-likely-to-kill">‘Dystopian’ tool aims to predict murder, 9 April</a>), the collection and automation of data has repeatedly led to the targeting of racialised and low-income communities, and must come to an end. This has been found by both Amnesty International in our <a href="https://www.amnesty.org.uk/files/2025-02/Automated%20Racism%20Report%20-%20Amnesty%20International%20UK%20-%202025.pdf?VersionId=JqCcTODw37yAXyINmAY6uAzrKEWucFF7">Automated Racism</a> report and by <a href="https://www.statewatch.org/news/2025/april/uk-ministry-of-justice-secretly-developing-murder-prediction-system/">Statewatch in its findings</a> on the “murder prediction” tool.</p> <p>For many years, successive governments have invested in data-driven and data-based systems, stating they will increase public safety – yet individual police forces and Home Office evaluations have found no compelling evidence that these systems have had any impact on reducing crime.</p> <p>Feedback loops are created by training these systems using historically discriminatory data, which leads to the same areas being targeted once again. These systems are neither revelatory nor objective. They merely subject already marginalised communities to compounded discrimination. They aren’t predictive at all, they are predictable – and dangerous.<br><strong>Ilyas Nagdee</strong><br><em>Amnesty International UK</em></p> <p>• The 2002 movie Minority&nbsp;Report was about a police unit that arrested people before they could commit crimes. Science fiction may be coming true. The unit’s head, played by Tom Cruise, was accused of “future murder” and had to go on the run. As we are finding out with AI tools, these programs have built-in limitations&nbsp;and errors.<br><strong>Bruce Higgins</strong><br><em>San Diego, California, US</em></p> <p>• Reading about the government’s “sharing data to improve risk assessment” project calls to mind the Thought Police in George Orwell’s Nineteen Eighty-Four.<br><strong>Geoff Walmsley</strong><br><em>Wirral, Merseyside</em></p> <p><strong><em>• Do you have a photograph you’d like to share with Guardian readers? If so, please </em></strong><a href="https://www.theguardian.com/artanddesign/2018/jan/15/share-your-best-photographs-of-the-week-with-us"><strong><em>click here</em></strong></a><strong><em> to upload it. A selection will be published in our </em></strong><a href="https://www.theguardian.com/community/series/readers--best-photographs"><strong><em>Readers’ best photographs galleries</em></strong></a><strong><em> and in the print edition on Saturdays.</em></strong></p>