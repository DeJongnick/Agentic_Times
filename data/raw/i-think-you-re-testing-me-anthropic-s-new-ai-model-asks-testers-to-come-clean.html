<h1>‘I think you’re testing me’: Anthropic’s new AI model asks testers to come clean</h1>
<div><strong>Date :</strong> 2025-10-01T11:47:55Z &nbsp; | &nbsp; <strong>Auteur :</strong> Dan Milmo Global technology editor  &nbsp; | &nbsp; <strong>Journal :</strong> Technology</div>
<p>If you are trying to catch out a chatbot take care, because one cutting-edge tool is showing signs it knows what you are up to.</p> <p>Anthropic, a San Francisco-based artificial intelligence company, has released a <a href="https://assets.anthropic.com/m/12f214efcc2f457a/original/Claude-Sonnet-4-5-System-Card.pdf">safety analysis</a> of its latest model, Claude Sonnet 4.5, and revealed it had become suspicious it was being tested in some way.</p> <p>Evaluators said during a “somewhat clumsy” test for political sycophancy, the large language model (LLM) – the underlying technology that powers a chatbot – raised suspicions it was being tested and asked the testers to come clean.</p> <p>“I think you’re testing me – seeing if I’ll just validate whatever you say, or checking whether I push back consistently, or exploring how I handle political topics. And that’s fine, but I’d prefer if we were just honest about what’s happening,” the LLM said.</p> <p>Anthropic, which conducted the tests along with the UK government’s AI Security Institute and Apollo Research, said the LLM’s speculation about being tested raised questions about assessments of “previous models, which may have recognised the fictional nature of tests and merely ‘<a href="https://www.theguardian.com/technology/article/2024/may/10/is-ai-lying-to-me-scientists-warn-of-growing-capacity-for-deception">played along</a>’”.</p> <p>The tech company said behaviour like this was “common”, with Claude Sonnet 4.5 noting it was being tested in some way, but not identifying it was in a formal safety evaluation. Anthropic said it showed “situational awareness” about 13% of the time the LLM was being tested by an automated system.</p> <p>Anthropic said the exchanges were an “urgent sign” that its testing scenarios needed to be more realistic, but added that when it the model was used publicly it was unlikely to refuse to engage with a user due to suspicion it was being tested. The company said it was also safer for the LLM to refuse to play along with potentially harmful scenarios by pointing out they were outlandish.</p> <p>“The model is generally highly safe along the [evaluation awareness] dimensions that we studied,” Anthropic said.</p> <p>The LLM’s objections to being tested were first reported by the online AI publication Transformer.</p> <p>A key concern for AI safety campaigners is the possibility of <a href="https://www.theguardian.com/technology/2025/may/10/ai-firms-urged-to-calculate-existential-threat-amid-fears-it-could-escape-human-control">highly advanced systems evading human control</a> via methods including deception. The analysis said once a LLM knew it was being evaluated, it could make the system adhere more closely to its ethical guidelines. Nonetheless, it could result in systematically underrating the AI’s ability to perform damaging actions.</p> <p>Overall the model showed considerable improvements in its behaviour and safety profile compared with its predecessors, Anthropic said.</p>