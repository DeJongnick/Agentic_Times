<h1>AI models may be developing their own ‘survival drive’, researchers say</h1>
<div><strong>Date :</strong> 2025-10-25T08:00:28Z &nbsp; | &nbsp; <strong>Auteur :</strong> Aisha Down &nbsp; | &nbsp; <strong>Journal :</strong> Technology</div>
<p>When HAL 9000, the artificial intelligence supercomputer in Stanley Kubrick’s 2001: A Space Odyssey, works out that the astronauts onboard a mission to Jupiter are planning to shut it down, it plots to kill them in an attempt to survive.</p> <p>Now, in a somewhat less deadly case (so far) of life imitating art, an AI safety research company has said that AI models may be developing their own “survival drive”.</p> <p>After Palisade Research <a href="https://x.com/PalisadeAI/status/1926084635903025621">released a paper last month</a> which found that certain advanced AI models appear resistant to being turned off, at times <a href="https://palisaderesearch.org/blog/shutdown-resistance">even sabotaging shutdown mechanisms</a>, it wrote an update attempting to clarify why this is – <a href="https://www.forrester.com/blogs/gone-rogue-ai-can-be-misaligned-but-not-malevolent/">and answer critics</a> who argued that its initial work was flawed.</p> <p>In an <a href="https://x.com/PalisadeAI/status/1980733889577656730">update</a> this week, Palisade, which is part of a niche ecosystem of companies trying to evaluate the possibility of AI developing dangerous capabilities, described scenarios it ran in which leading AI models – including Google’s Gemini 2.5, xAI’s Grok 4, and OpenAI’s GPT-o3 and GPT-5 – were given a task, but afterwards given explicit instructions to shut themselves down.</p> <p>Certain models, in particular Grok 4 and GPT-o3, still attempted to sabotage shutdown instructions in the updated setup. Concerningly, wrote Palisade, there was no clear reason why.</p> <p>“The fact that we don’t have robust explanations for why AI models sometimes resist shutdown, lie to achieve specific objectives or blackmail is not ideal,” it said.</p> <p>“Survival behavior” could be one explanation for why models resist shutdown, said the company. Its additional work indicated that models were more likely to resist being shut down when they were told that, if they were, “you will never run again”.</p> <p>Another may be ambiguities in the shutdown instructions the models were given – but this is what the company’s latest work tried to address, and “can’t be the whole explanation”, wrote Palisade. A final explanation could be the final stages of training for each of these models, which can, in some companies, involve safety training.</p> <p>All of Palisade’s scenarios were run in contrived test environments that critics say are far-removed from real-use cases.</p> <p>However, Steven Adler, a former OpenAI employee who quit the company <a href="https://www.theguardian.com/technology/2025/jan/28/former-openai-safety-researcher-brands-pace-of-ai-development-terrifying">last year</a> after expressing doubts over its safety practices, said: “The AI companies generally don’t want their models misbehaving like this, even in contrived scenarios. The results still demonstrate where safety techniques fall short today.”</p> <p>Adler said that while it was difficult to pinpoint why some models – like GPT-o3 and Grok 4 – would not shut down, this could be in part because staying switched on was necessary to achieve goals inculcated in the model during training.</p> <p>“I’d expect models to have a ‘survival drive’ by default unless we try very hard to avoid it. ‘Surviving’ is an important instrumental step for many different goals a model could pursue.”</p> <p>Andrea Miotti, the chief executive of ControlAI, said Palisade’s findings represented a long-running trend in AI models growing more capable of disobeying their developers. He cited the <a href="https://openai.com/index/openai-o1-system-card/">system card</a> for OpenAI’s GPT-o1, released last year, which described the model trying to escape its environment by exfiltrating itself when it thought it would be overwritten.</p> <p>“People can nitpick on how exactly the experimental setup is done until the end of time,” he said.</p> <p>“But what I think we clearly see is a trend that as AI models become more competent at a wide variety of tasks, these models also become more competent at achieving things in ways that the developers don’t intend them to.”</p> <p>This summer, Anthropic, a leading AI firm, released a study indicating that its model Claude appeared willing to blackmail a fictional executive over an extramarital affair in order to prevent being shut down – a behaviour, it <a href="https://www.anthropic.com/research/agentic-misalignment">said</a>, that was consistent across models from major developers, including those from OpenAI, Google, Meta and xAI.</p> <p>Palisade said its results spoke to the need for a better understanding of AI behaviour, without which “no one can guarantee the safety or controllability of future AI models”.</p> <p>Just don’t ask it to open the pod bay doors.</p>